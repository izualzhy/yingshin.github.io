---
title: "实时任务和离线任务"
date: 2024-03-03 21:12:29
tags: Pensieve
---

## 1.时间的边界

### 1.1. T+1

时间分两种，处理时间和事件时间。大部分情况，数据处理都会选择事件时间。

以离线的天级 Hive 表任务为例，我们看看是如何产出 T+1 的数据的。

T+1 00:00 是处理时间，假定 A 表 Tday 的数据在 00:05 完全到达，B 表 T-day 的数据在 01:05 完全到达。   

数据完全到达后，开始 merge Tday 的增量数据，然后根据需要生成全量表或者拉链表。可能的时间轴：

1. 00:05 -> 00:30: merge A 表 Tday 的增量数据，记录到 A-inc
2. 00:30 -> 01:35: merge A-inc + A-base，记录到 A 表 Tday 分区
3. 01:05 -> 01:10: merge B 表 Tday 的增量数据，记录到 B-inc
4. 01:10 -> 01:50: merge B-inc + B-base，记录到 B 表 Tday 分区
5. 01:50: 开始执行 SQL A + B -> C，产出 C 表的 Tday 分区

这里为离线数仓任务的开发建立了一个非常友好的模型：**SQL 处理的是需要的全部数据**。

基于这个易用且成熟的模型，基础设施上只需要确保两点：   
1. 如何判断数据是否完全到达？需要考虑生产环境一直没有数据的情况，因此依赖 FileAgent/CDC 的心跳包。整个数据流是保序的，当 T+1 day 的心跳包到达，就可以认为 T day 的数据已经采集完成。  
2. 如何编排上述任务？依赖于任务调度系统，相比于 K8S jobs、Linux crontab，大数据的任务调度系统，在功能上最强大的两点之一便是编排任务 DAG 的能力。通过任务编排，离线任务简单划分为了依赖检查和任务执行阶段。

### 1.2. Mirco Batch

离线是处理1天的数据，微批处理时间间隔更小，比如10分钟。因此从离线过渡到微批，似乎更加顺理成章一些。

[08:00, 08:10] 区间的数据，A 表在 08:11 完全到达，B 表在 08:16 完全到达。如果完全复用离线处理的思路：(A-base + A-inc) JOIN (B-base + B-inc)，那数据量就太大了，包含了很多的无用计算&存储，同时对下游，也不是 Micro Batch，而是全量数据。

因此，构建的目标应当是产出该 Mirco Batch 内产生变化的数据。

一个初步的想法是：`(A-inc JOIN B-base) UNION (B-inc JOIN A-base)`，不过 A-inc 和 B-inc 的数据可能也有关联，因此修正为：
**(A-inc JOIN (B-base + B-inc)) UNION (B-inc JOIN (A-base + A-inc))**. 这样 SQL 产出的数据就是需要给到下游的全部增量数据。

具体的：
1. B-base + B-inc 如何实现？需要一个支持 upsert 的存储系统     
2. A-inc JOIN B-merged：需要该存储系统支持点查      
3. 同样依赖调度系统编排任务  
4. A-inc 应该如何获取？通过存储系统字段/用户字段，或者计算引擎的 windows/batch/binlog 处理，从实现复杂度上前者更合适。

使用事件时间的好处，是系统对外清晰，当前批次处理完成，那么 <= 08:10 的数据就都处理完成。如果放到生产环境，还需要考虑使用事件时间时，一次性要处理的数据可能过多。  

反过来，如果 T+1 关注的是昨天(\<今天00:00)的指标，当开始追求微批处理，我们关注的其实是时效性，而不是这个数据是否一定 <= 08:10。这种情况下，也可以考虑使用处理时间。

上一节在处理 A-inc 时，还有一个默认对用户屏蔽的点，就是删除数据。在当前系统，需要 A 表只能 markDel，如果物理删除的话依赖从 binlog 读取增量数据。因此，基于 Mircro Batch 模型，SQL 用户也需要考虑删除数据如何传递的问题。  

### 1.3. Stream

## 2. 平台

## 3. 稳定性要求

## 4. 计算引擎

