---
title: "linkedin/coral 代码速读"
date: 2023-12-10 15:24:02
tags: coral
---

## 1. SQL 重写

大数据领域，随着数据量变大、时效性要求越来越多样化，SQL 计算引擎也越来越多，从原来的 HiveSQL，到如今的 Presto/Trino、Flink、Spark。同时，随着 storage format、table format、table schema 各个方向的精细发展，SQL 的形式也越来越多，短期内也很难出现事实上的统一标准。

SQL 往往需要在不同执行计算引擎间变更，比如：
1. 分析师的 HiveSQL 运行很慢，希望能够修改为 TrinoSQL 执行    
2. 数仓工程师的 HiveSQL，希望能够统一修改为 SparkSQL 执行   

由于不同 SQL 间存在语法差异，SQL 工程师就需要重新学习新 SQL 的特有语法，类比于开发工程师重新学习一门编程语言。

SQL 间语法差异，比如 HiveSQL 和 SparkSQL 的兼容性的例子<sup>3</sup>，此外还有诸如 order by 别名、hash方法、返回值、grouping sets 等。HiveSQL 和 PrestoSQL 也是如此。python 里类似的工具比较多，比如[tobymao/sqlglot](https://github.com/tobymao/sqlglot).

另外，笔者看到的实际情况，往往是一份数据多处存储。例如对于一份数据，ClickHouse 往往以宽表的形式存在，Hive 则定义成多张表(或者 Join 成 view)。如果想要统一 SQL，就需要建立 access layer:

```
               ┌──────────────┐
               │ access layer │
               └──────┬───────┘
                      │
                      │
   ┌─────────┬────────┼───────────┬──────────────┐
   │         │        │           │              │
   │         │        │           │              │
┌──┴──┐   ┌──┴──┐   ┌─┴──┐   ┌────┴──┐    ┌──────┴───┐
│Trino│   │Kafka│   │Hive│   │Iceberg│    │ClickHouse│
└─────┘   └─────┘   └────┘   └───────┘    └──────────┘
```

这一层是逻辑上的表，通过元数据关联到具体存储层（Trino、Kafka、Hive、Iceberg、CH、etc.）的表。

这里有点像 Flink 之前一直推广的流批一体<sup>1</sup>，不同的是其思路是使用 FlinkSQL 统一所有计算，而不是通过 SQL 按不同引擎重写实现。

## 2. Coral 介绍

linkedin 对 Coral 的定义是：**SQL translation, analysis, and rewrite engine**<sup>2</sup>.   
Coral 的核心思路是将 HiveSQL 转为 calcite 的 RelNode(文档里的Coral IR?)，然后再转为其他方言的 SQL.

![coral-hive-to-IR](/assets/images/coral/coral-hive-to-IR.png)

由于存在 Dali views 这一层，因此 coral-hive 负责读取其中的元信息：database/table/view 名、schema 等。使用 Hive 的能力将 sql 转为 AST(Abstract Syntax Tree)，然后遍历转换为 calcite 的 RelNode.

_注：关于 RelNode 介绍[Calcite笔记之三：处理流程的代码例子](https://izualzhy.cn/calcite-example)_

![coral-IR-to-presto](/assets/images/coral/coral-IR-to-presto.png)

这个图里，则是将 RelNode 重写为 PrestoSQL

## 3. Coral 代码

以单测方法作为入口:

```java
public class HiveToTrinoConverterTest {
   @Test
   public void testTypeCastForCardinalityFunction() {
      RelToTrinoConverter relToTrinoConverter = TestUtils.getRelToTrinoConverter();

      RelNode relNode = TestUtils.getHiveToRelConverter().convertSql("SELECT size(ARRAY (1, 2))");
      String targetSql = "SELECT CAST(CARDINALITY(ARRAY[1, 2]) AS INTEGER)\n" + "FROM (VALUES  (0)) AS \"t\" (\"ZERO\")";
      String expandedSql = relToTrinoConverter.convert(relNode);
      assertEquals(expandedSql, targetSql);
   }
}
```

该方法处理的也是在之前[如何用 ANTLR 解析和重写SQL](https://izualzhy.cn/antlr4)介绍过的 SQL 例子。代码分为两个步骤：
1. hiveSQL -> relNode: 通过`RelNode HiveToRelConverter.convertSql(String sql)`实现
2. relNode -> trinoSQL: 通过`RelToTrinoConverter.convert(RelNode relNode)`实现

```java
public abstract class ToRelConverter {
    public RelNode convertSql(String sql) {
        return toRel(toSqlNode(sql));
    }
}
```

转换的流程：`sql -> SqlNode -> RelNode`

```java
public class ParseTreeBuilder extends AbstractASTVisitor<SqlNode, ParseTreeBuilder.ParseContext> {

  public SqlNode process(String sql, @Nullable Table hiveView) {
    ParseDriver pd = new CoralParseDriver();
    try {
      ASTNode root = pd.parse(sql);
      return processAST(root, hiveView);
    } catch (ParseException e) {
      throw new RuntimeException(e);
    }
  }
}
```

`CoralParseDriver`跟 Hive 原生的`org.apache.hadoop.hive.ql.parse.ParseDriver`基本一致，调用了 antlr 的 HiveParser/HiveLexer 解析出 ASTNode.

`processAST`遍历 ASTNode 逐个转换为 SqlNode，例如对于`HiveParser.TOK_FUNCTION`，调用`visitFunction`方法：

```java
public class ParseTreeBuilder extends AbstractASTVisitor<SqlNode, ParseTreeBuilder.ParseContext> {
   protected SqlNode visitFunction(ASTNode node, ParseContext ctx) {
      return visitFunctionInternal(node, ctx, null);
   }

   private SqlNode visitFunctionInternal(ASTNode node, ParseContext ctx, SqlLiteral quantifier) {
      ArrayList<Node> children = node.getChildren();
      checkState(children.size() > 0);
      ASTNode functionNode = (ASTNode) children.get(0);
      String functionName = functionNode.getText();
      List<SqlNode> sqlOperands = visitChildren(children, ctx);
      Function hiveFunction = functionResolver.tryResolve(functionName, ctx.hiveTable.orElse(null),
              // The first element of sqlOperands is the operator itself. The actual # of operands is sqlOperands.size() - 1
              sqlOperands.size() - 1);

      // Special treatment for Window Function
      SqlNode lastSqlOperand = sqlOperands.get(sqlOperands.size() - 1);
      if (lastSqlOperand instanceof SqlWindow) {
         // ...
      }

      if (functionName.equalsIgnoreCase("SUBSTRING")) {
         // ...
      }

      return hiveFunction.createCall(sqlOperands.get(0), sqlOperands.subList(1, sqlOperands.size()), quantifier);
   }

}
```
该方法首先遍历 children 转换为 SqlNode，然后调用 hiveFunction 创建对应的`SqlCall`对象，例如对于`ARRAY (1, 2)`，转换后的对象如图：

![SqlCall](/assets/images/coral/sqlcall.png)

接下来基本复用[calcite SqlToRelConverter](https://izualzhy.cn/calcite-arch#4-%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B)转化为 RelNode 结构。

RelNode 转化为 SQL 的过程，跟上述流程相反。

## 4. 参考资料

1. [流批一体技术在天猫双11的应用](https://files.alicdn.com/tpsservice/abbaec7b252c69ff7f721663f40c6eda.pdf)
2. [Coral: A SQL translation, analysis, and rewrite engine for modern data lakehouses](https://engineering.linkedin.com/blog/2020/coral)
3. [SparkSql与HiveSql的兼容性——踩过的十大坑](https://stefanxiepj.github.io/archives/bbca4555.html)
   在之前介绍过[使用 ANTLR4 重写 HiveSQL 为 PrestoSQL 的例子]，毫无疑问这里的工作量很大。